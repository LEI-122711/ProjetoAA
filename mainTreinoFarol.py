import matplotlib.pyplot as plt
import math
from Ambiente.AmbienteFarol import AmbienteFarol
from Agentes.AgenteFarolQLearning1 import AgenteFarolQLearning
from Sensores.SensorDirecaoAlvo import SensorDirecaoAlvo
from Sensores.SensorLocalFarol import SensorLocalFarol
from Sensores.SensorProximidadeObst√°culo import SensorProximidadeObst√°culo
from Simulador import Simulador
from Visualizador import Visualizador
import random
import numpy as np


def treinar():
    print("--- IN√çCIO DO TREINO Q-LEARNING ---")

    # 1. Configura√ß√£o Inicial
    agente = AgenteFarolQLearning(
        learning_rate=0.1,
        discount_factor=0.9,
        exploration_rate=1,
    )

    # IMPORTANTE: Instalar os DOIS sensores
    # O agente precisa de saber a Dire√ß√£o E os Obst√°culos
    sensor_gps = SensorDirecaoAlvo()
    sensor_obs = SensorProximidadeObst√°culo(raio=1)  # Raio 1 v√™ 3x3

    agente.instala(sensor_gps)
    agente.instala(sensor_obs)

    historico_passos = []
    EPISODIOS = 20000  # Quantas vezes ele vai tentar
    target_epsilon = 0.01
    start_epsilon = 1.0
    #decay_steps = EPISODIOS * 0.95

    #DECAY_RATE = math.pow(target_epsilon / start_epsilon, 1 / decay_steps)

    # 2. Ciclo de Epis√≥dios
    for ep in range(EPISODIOS):
        passos = []
        amb = AmbienteFarol()
        amb.add_obstaculo(8, 7)  # Parede simples
        amb.add_obstaculo(7, 7)

        amb.add_agente(agente, x=0, y=0)

        # Decay do Epsilon
        agente.epsilon = start_epsilon - ep * (start_epsilon - target_epsilon) / (EPISODIOS - 1)

        # --- L√ìGICA DE "ESPREITAR" ---
        espreitar = (ep == 0) or ((ep + 1) % 5000 == 0) or (ep == EPISODIOS - 1)

        sim = Simulador(amb, [agente])

        if espreitar:
            print(f"üëÄ A Visualizar Epis√≥dio {ep + 1} (Epsilon: {agente.epsilon:.2f})...")
            # Agora j√° n√£o vai dar erro aqui
            sim.visualizador = Visualizador(amb)
        else:
            sim.visualizador = None

        sim.executar_simulacao()
        historico_passos.append(sim.passo)

        if (ep + 1) % 50 == 0:
            print(f"Epis√≥dio {ep + 1}/{EPISODIOS} completado em {sim.passo} passos.")

    print("--- TREINO CONCLU√çDO ---")

    agente.record_data()

    # 3. Mostrar Gr√°fico (Para o Relat√≥rio)
    passos_np = np.array(historico_passos)

    # Calcular M√©dia M√≥vel (Janela de 100 epis√≥dios)
    # Isto suaviza o gr√°fico e mostra a tend√™ncia real
    media_movel = np.convolve(passos_np, np.ones(100) / 100, mode='valid')

    plt.figure(figsize=(10, 5))

    # Plota os dados brutos em cinzento claro (fundo)
    plt.plot(historico_passos, alpha=0.3, color='gray', label='Bruto')

    # Plota a m√©dia m√≥vel em azul forte (tend√™ncia)
    plt.plot(media_movel, color='blue', label='M√©dia M√≥vel (100 eps)')

    plt.xlabel('Epis√≥dio')
    plt.ylabel('Passos')
    plt.title('Curva de Aprendizagem (Suavizada)')
    plt.legend()
    plt.show()

    # 4. Teste Final (COM VISUALIZADOR)
    print("A executar teste visual com o c√©rebro treinado...")
    agente.learning_mode = False  # Parar de aprender/explorar

    amb_final = AmbienteFarol()
    amb_final.add_obstaculo(8, 7)
    amb_final.add_obstaculo(7, 7)
    amb_final.add_obstaculo(6, 7)

    amb_final.add_agente(agente, x=0, y=0)

    # Aqui o visualizador vai abrir
    sim_final = Simulador(amb_final, [agente])
    sim_final.executar_simulacao()


if __name__ == "__main__":
    treinar()